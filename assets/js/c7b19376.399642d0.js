"use strict";(self.webpackChunkllmos_ai=self.webpackChunkllmos_ai||[]).push([[613],{6063:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>m,contentTitle:()=>s,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>l});var o=t(4848),r=t(8453);const i={sidebar_position:2,title:"Model Serving"},s=void 0,a={id:"user_guide/llm_management/serve",title:"Model Serving",description:"LLMOS helps you to create a model serving through the ModelServe resource, which is a intuitive interface to specify a model serving configurations",source:"@site/docs/user_guide/llm_management/serve.md",sourceDirName:"user_guide/llm_management",slug:"/user_guide/llm_management/serve",permalink:"/docs/user_guide/llm_management/serve",draft:!1,unlisted:!1,editUrl:"https://github.com/llmos-ai/llmos.ai/tree/main/docs/docs/user_guide/llm_management/serve.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2,title:"Model Serving"},sidebar:"tutorialSidebar",previous:{title:"Notebooks",permalink:"/docs/user_guide/llm_management/notebooks"},next:{title:"Storage",permalink:"/docs/category/storage"}},m={},l=[{value:"Environment Variables",id:"environment-variables",level:3},{value:"Example YAML",id:"example-yaml",level:3}];function c(e){const n={code:"code",h3:"h3",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["LLMOS helps you to create a model serving through the ",(0,o.jsx)(n.code,{children:"ModelServe"})," resource, which is a intuitive interface to specify a model serving configurations\n(resource requirements, setup commands, run commands, file mounts, storage mounts, and so on)."]}),"\n",(0,o.jsxs)(n.p,{children:["LLMOS provides a intuitive interface to create a model serving through the ",(0,o.jsx)(n.code,{children:"ModelService"})," resource. It supports the following features:"]}),"\n",(0,o.jsx)(n.h3,{id:"environment-variables",children:"Environment Variables"}),"\n",(0,o.jsx)(n.h3,{id:"example-yaml",children:"Example YAML"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-yaml",children:'# modelservice yaml file\napiVersion: ml.llmos.ai/v1\nkind: ModelService\nmetadata:\n  name: gemma-2-9b-it\nspec:\n  replicas: 1\n  model: google/gemma-2-9b-it\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    spec:\n      runtimeClassName: nvidia\n      containers:\n        - name: server\n          image: vllm/vllm-openai:latest\n          ports:\n            - containerPort: 8000\n              protocol: TCP\n              name: http\n          env:\n            - name: HUGGING_FACE_HUB_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: huggingface-token\n                  key: token\n          volumeMounts:\n            - mountPath: /dev/shm\n              name: memory-cache\n            - mountPath: /root/.cache/huggingface\n              name: hf-dir\n          resources:\n            limits:\n              nvidia.com/gpu: "1"\n      volumes:\n        - emptyDir:\n            medium: Memory\n          name: memory-cache\n        - name: hf-dir\n          persistentVolumeClaim:\n            claimName: gemma-2-9b-it-hf-dir # keep the name to be consistent with the spec.volumes name\n  volumes:\n    - name: gemma-2-9b-it-hf-dir\n      spec:\n        accessModes:\n          - ReadWriteOnce\n        resources:\n          requests:\n            storage: 20Gi\n  serviceType: ClusterIP\n'})})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var o=t(6540);const r={},i=o.createContext(r);function s(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);